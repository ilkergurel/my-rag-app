{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from collections import defaultdict\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import HttpClient\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import fitz\n",
    "\n",
    "# Recursively find all .pdf files in subdirectories\n",
    "\n",
    "def get_docs(directory):\n",
    "    file_dict = defaultdict(list)\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf') or  file.lower().endswith('.epub'):  # Check if it's a .pdf or .epub file         \n",
    "                file_path =  os.path.join(root, file)                    #Obtain filepath and remove path length control with \\\\?\\\\\n",
    "                file_name, file_ext = os.path.splitext(file)                         #Obtain filename without extension                                   \n",
    "                parent_path = os.path.basename(os.path.dirname(file_path))           #Obtain parent directory\n",
    "                file_size = os.path.getsize(file_path)                               #Obtain file size\n",
    "                file_dict[file_name].append([file_path, parent_path, file_size, file_ext])\n",
    "\n",
    "\n",
    "    return file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of element in chromadb: 3076636\n",
      "Number of database elements: 3076636\n",
      "Number of documents  in harddisk: 1963\n",
      "Number of documents in database: 1863\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client (PersistentClient for persistent storage)\n",
    "client = chromadb.PersistentClient(path=\"D:\\Bilgi\\__Databases\\Bilgi_my_chroma_db_with_langchain\")\n",
    "\n",
    "# Access your collection by name\n",
    "collection = client.get_collection(\"my-rag-db\")\n",
    "\n",
    "total_items = collection.count()\n",
    "print(f\"Total number of element in chromadb: {total_items}\")\n",
    "\n",
    "batch_size = 100000\n",
    "\n",
    "list_items_info = []\n",
    "counter = 0\n",
    "documents = defaultdict(list)\n",
    "\n",
    "for offset in range(0, total_items, batch_size):\n",
    "    # Retrieve a batch of data\n",
    "\n",
    "    print(f\"Counter: {counter} / {total_items}  \", end=\"\\r\")\n",
    "\n",
    "    batch = collection.get(\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "        limit=batch_size,\n",
    "        offset=offset\n",
    "    )\n",
    "\n",
    "    # Process each item in the batch\n",
    "    for doc, meta, id in zip(batch[\"documents\"], batch[\"metadatas\"], batch[\"ids\"]):\n",
    "        # Replace the following line with your processing logic\n",
    "        #print(f\"Document: {doc}\\nMetadata: {meta}\\nIds: {id}\\n\")\n",
    "\n",
    "        path = meta.get(\"source\", \"Unknown\")\n",
    "        doc_name = path.split(\"\\\\\")[-1]\n",
    "        page = meta.get(\"page\", \"Unknown\")\n",
    "        total_pages = meta.get(\"total_pages\", \"Unknown\")\n",
    "        chunk_length = len(doc)\n",
    "\n",
    "        list_items_info.append([id, chunk_length, path, doc_name, page, total_pages, doc])\n",
    "\n",
    "        #Get the documents which are in the database\n",
    "        if not path in documents:\n",
    "            documents[path].append([chunk_length, total_pages])\n",
    "\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "\n",
    "print(f\"Number of database elements: {len(list_items_info)}\")\n",
    "\n",
    "directory = \"D:\\\\Bilgi\"\n",
    "file_dict = get_docs(directory)\n",
    "\n",
    "print(f\"Number of documents  in harddisk: {len(file_dict)}\")\n",
    "\n",
    "print(f\"Number of documents in database: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing docs: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\AudioProcessing\\\\(ebook) Prentice Hall - Digital Processing Of Speech Signals (Lawrence R. Rabiner & Ronald W. Schafer) (scanned).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\AudioProcessing\\\\[eBook ENG] [DSP-audio] - Digital Audio Signal Processing - Zolzer.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Big Data\\\\Data Mining Process with Neural Networks - Joseph P Bigus.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Biography\\\\Mao Tse-Tung - A Biography By Ross Terrill.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Business\\\\Amos Tversky And Daniel Kahneman - Prospect Theory - An Analysis Of Decision Under Risk (1979).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Business\\\\MIT_Managing_the_Total_Customer_Experience.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\(ebook - electronics) - Practical RF Circuit Design for Modern Wireless Systems - Vol. 1- Passive Circuits & Systems (Besser Gilmore 2003).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Allen, Holberg - CMOS Analog Circuit Design second edition.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Analog Circuit Design - Sansen & Huijsing.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Analog Circuit Design - Volt Electronics - Mixed-Mode Systems - Low-Noise and RF Power Amplifiers for Telecommunication.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Analog Integrated Circuit Design - Johns & Martin.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Handbook of Analog Circuit Design - D. Feucht (AP) WW.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Prentice Hall - Phase-Locked Loop Circuit Design.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Circuits\\\\Secrets of Rf Circuit Design (McGraw-Hill Joseph J. Carr).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Deep Learning\\\\Artificial Intelligence - IEEE Artificial Neural Networks A Tutorial.PDF',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Deep Learning\\\\Learning from data - A short course - Abu-Mostafa.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Deep Learning\\\\Natural Language Processing\\\\2009-natural_language_processing_with_python.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Deep Learning\\\\Natural Language Processing\\\\Speech and Language Processing - An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Deep Learning\\\\Natural Language Processing\\\\W. Chou - Pattern Recognition in Speech and Language Processing.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Design & Manufacturing\\\\3642231861 Human-Computer Systems Interaction. Backgrounds and Applications 2. Part 1 (Springer, AISC 98, 2012).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Design & Manufacturing\\\\Springer - Human-Computer Interaction - Users And Applications, 14 Conf , Hci 2011, Part Iv (2011) [9783642216183].pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\DevOps\\\\The Phoenix Project_ A Novel About IT, DevOps, and Helping Your Business Win - Kim, Gene.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Economy\\\\(Ebook) Technical Analysis of the Financial Markets - Comprehensive Guide to Trading Methods & Application by John J. Murphy (1999).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Influencing & Communication & Relationships\\\\Business - McGraw Hill - Customer Relationship Management - CRM.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Influencing & Communication & Relationships\\\\Negotiation - Tony Robbins - The Power To Influence (Sales Mastery) - Backtrack Notes (175P).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Influencing & Communication & Relationships\\\\Nlp - Kevin Hogan - The Psychology Of Persuasion How To Persuade Others To Your Way Of Thinking.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Influencing & Communication & Relationships\\\\The Psychology Of Persuasion How To Persuade Others To Your Way Of Thinking - Kevin Hogan.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Investing\\\\Benjamin Graham & David Dodd - Security analysis.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Investing\\\\Buffett, Warren. Fundamentals of Managerial Economics.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Investing\\\\Finance.Modern Portfolio Theory and Investment Analysis-6th ed.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Investing\\\\Fooled By Randomness - Taleb, Nassim (2007).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Investing\\\\Invest - How to Make Money Trading Stocks and Commodities - Sranko, George.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Investing\\\\__The Little Book Of Common Sense Investing (BOGLE John 2007 W.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\Harvard Business Review - Leadership And Strategy For The Twenty-First Century - 022008.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\How It Works Book of Great Inventors & Their Creations 2015 Edition - Imagine Publishing (Imagine Publishing;How It Works Bookazine;2015;9781785460203;eng).pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\John C Maxwell - There's No Such Thing As Business Ethics.pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\Leadership & Strategy - Harvard Business Review.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\psychology - NLP - Robert Dilts - Visionary Leadership Skills.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\PSYCHOLOGY - NLP.-.Robert.Dilts.-.Visionary.Leadership.Skills.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\Schelling - The Strategy Of Conflict.pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Leadership & Organization\\\\The Leader's Guide To Lateral Thinking Skills - Unlocking The Creativity And Innovation In You And Your Team.pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Marketing\\\\Longman - Market Leader - Advanced Business English Course Book.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Martial Arts\\\\Donn F Draeger - The Martial Arts And Ways Of Japan Volume One - Classical Budo..pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Martial Arts\\\\Donn F Draeger - The Martial Arts And Ways Of Japan Volume Two - Classical Budo.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Martial Arts\\\\Randy Gonzalez - Social Survival Tactics. A guide to basic self-defense and personal safety strategy.pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Politics\\\\(Ebook - Pdf) Sun Tzu's Art Of War.pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Politics\\\\History of the Second World War ( Basil Liddell Hart, 1970).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Politics\\\\Leo Strauss - Machiavelli Prince.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Politics\\\\Skinner - Machiavelli A Very Short Introduction.pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Politics\\\\Strategy And Tactics No 075 - Napoleon's Art Of War - Eylau And Dresden [Jul-Aug 79].pdf\",\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Politics\\\\Strauss, Leo -Machiavelli's Intention The Prince.pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Science\\\\The Character Of Physical Law (Mit Press) - R P Feynman(Mr)(Acro9).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Self Improvement\\\\( EBOOK - PDF - ENG) Covey, Stephen - The Seven Habits Of Highly Effective People.pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Self Improvement\\\\(ebook) Reinventing Yourself. How To Become The Person You've Always Wanted To Be by Steve Chandler.pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Self Improvement\\\\Napoleon Hill - Keys To Success The 17 Principles Of Personal Achievement (1994).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Self Improvement\\\\Napoleon Hill - Think And Grow Rich.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Self Improvement\\\\Taleb, Nassim Nicholas - Fooled By Randomness; The Hidden Role Of Chance In Markets And Life _.pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Self Improvement\\\\•••Napoleon Hill   Napoleon Hill's Keys to Success  The 17 Principles of Personal Achievement (1997, Plume)             JM20352 Col .pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\(Control - DSP) - Digital Signal Processing 3ed Proakis, Manolakis -.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Calculus.A.Complete.Course,.Robert.A..Adams,.Christopher.Essex,.7ed,.Pearson,.2010.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Digital Signal Processing - Proakis and Manolakis - 4th.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Information Theory, Probability, and Neural Networks - D. J. C. MacKay.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\M. H. Hayes - Statistical Digital Signal Processing and Modeling (Wiley, 1996).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Manolakis D G , Et Al(2005) Statistical And Adaptive Signal Processing Spectral Estimation, Signal Modeling, Adaptive Filtering, And Array Processing(806S).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\m_hayes_statistical_digital_signal_proc_part_1.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Nonlinear Programming - Theory And Algorithms 3Rd Ed - M Bazaraa, Et Al , (Wiley, 2006) Ww.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\S. M. Kay - Fundamentals of Statistical Signal Processing Estimation Theory - Problem Solutions (Prentice-Hall).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Signals and Systems [Alan V.Oppenheim, Alan S.Willsky, Hamid Nawad, with S.Hamid (Pearson Education, 1998)].pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\Statistical and Adaptive Signal Processing.. Spectral Estimation, Signal Modeling, Adaptive Filtering, and Array Processing(2005).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\[DSP - audio] - Digital Audio Signal Processing - Zolzer.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Signal Processing\\\\__Adaptive-signal-processing_widrow.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Sociology\\\\A Sociology of Globalization - Saskia Sassen.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Sociology\\\\Anthony Giddens - Politics, Sociology And Social Theory Stanford 1995.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Sociology\\\\Gladwell, Malcolm Little Brown, The Tipping Point - How Little Things Can Make A Big Difference.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Sociology\\\\Max Weber-The Religion of India_ The Sociology of Hinduism and Buddhism (2000).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Sociology\\\\Sassen, Saskia - A Sociology of Globalization [2007].pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Creativity - Maximize Your Brainpower - 1000 New Ways To Boost Your Mental Fitness.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Creativity - The Discipline of Innovation. By Drucker, Peter.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Creativity - The Six Values Medals The Essential Tool For Success In The 21St Century - Edward De Bono.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Harvard Business Review Peter Drucker - The Discipline of Innovation.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Peter Drucker - Creativity - The Discipline of Innovation.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Story Telling\\\\Writing - Character Development And Storytelling For Games.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\SYSTEM ENGINEERING\\\\Graphic Design Thinking.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\SYSTEM ENGINEERING\\\\Norman, Donald - The Design Of Everyday Things.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\SYSTEM ENGINEERING\\\\The Design of Everyday Things - Donald A. Norman.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Time Management\\\\Accelerated Learning (Stephen Covey).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\Anatomi Konu Kitabı ( PDFDrive.com ).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\Erol Göka - Türk Grup Davranışı.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\Halil İnalcık - Doğu Batı.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\Halil İnalcık - Osmanlıda Devlet Hukuk Adalet.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\İlber Ortaylı - Cumhuriyetin İlk Yüzyılı 1923-2023.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\Crucial Conversations.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\Executive Book Summary - Peter Drucker - Innovation And Entrepreneurship (Powertalk, Book Summary).pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\Handbook Of Innovation.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\Psychology - Oxford Handbook of Innovation.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\Six Sigma Trends Six Sigma Leadership And Innovation Using Triz.pdf',\n",
       " \"E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\The Art Of Innovation - Lessons In Creativity From Ideo, America's Leading Design Firm.pdf\",\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\__Browne,Keeley - Asking The Right Questions, A Guide To Critical Thinking, 8Th Ed.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\__Peter Drucker - Creativity - The Discipline of Innovation.pdf',\n",
       " 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\İş-Ge\\\\__Peter Drucker - Innovation And Entrepreneurship (Powertalk, Resumido).pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find missing docs after document chunks extraction\n",
    "directory = \"D:\\\\Bilgi\"\n",
    "file_dict = get_docs(directory)\n",
    "\n",
    "missing_docs = []\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for file, info in file_dict.items():\n",
    "    print(counter, end=\"\\r\")\n",
    "    counter += 1\n",
    "\n",
    "    filepath = file_dict[file][0][0]\n",
    "    doc_found = False\n",
    "    for item in list_items_info:\n",
    "        doc_name_in_list = item[2]\n",
    "        if filepath == doc_name_in_list:\n",
    "            doc_found = True\n",
    "            break\n",
    "        #print(filepath , \" - \", doc_name_in_list)\n",
    "    if not doc_found:\n",
    "        missing_docs.append(filepath)\n",
    "\n",
    "print(f\"Number of missing docs: {len(missing_docs)}\")\n",
    "\n",
    "\n",
    "missing_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error flag: False, warning flag: False\n",
      "Text: [Document(metadata={'producer': \"Relais Int'l and AIS ImagePDF 1.06\", 'creator': 'PyPDF', 'creationdate': '2003-03-06T06:57:40-05:00', 'moddate': '2007-03-18T17:52:25+01:00', 'source': 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Creativity - The Discipline of Innovation. By Drucker, Peter.pdf', 'total_pages': 7}, page_content='\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c')]\n",
      "Number of chunks: 0\n",
      "Total number of element in chromadb: 0\n",
      "Number of elements: 0\n",
      "Number of found docs: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full pipeline for a single document -- for testing\n",
    "\n",
    "def create_batches(data, ids, batch_size):\n",
    "    \"\"\"Yield successive batch_size-sized chunks from data.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size], ids[i:i + batch_size]\n",
    "\n",
    "\n",
    "file_counter = 0\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",  # Server's hostname or IP address\n",
    "    port=8000,         # Port number the server is listening on\n",
    "    settings=Settings(),\n",
    ")\n",
    "collection = client.get_or_create_collection(name=\"my-rag-db\") \n",
    "\n",
    "embedding_function = OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\",\n",
    ")\n",
    "\n",
    "#chroma run --host localhost --port 8000 --path D:/my_chroma_db_with_langchain2\n",
    "\n",
    "file_path = \"D:\\\\Bilgi\\\\Kitaplar\\\\Startup\\\\Creativity - The Discipline of Innovation. By Drucker, Peter.pdf\"\n",
    "\n",
    "error_flag = False\n",
    "warning_flag = False\n",
    "try:\n",
    "    loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "    doc = loader.load()      \n",
    "except Warning as w:\n",
    "    warning_flag = True\n",
    "    #warning_logger.warning(f\"{file_counter} -- {file_path}: {w}\")\n",
    "except Exception as e:\n",
    "    # Capture the exception and traceback\n",
    "    error_flag = True      \n",
    "    #error_logger.error(f\"{file_counter} -- {file_path}: {e}\", exc_info=True)\n",
    "\n",
    "file_counter += 1  \n",
    "print(f\"Error flag: {error_flag}, warning flag: {warning_flag}\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=40000, chunk_overlap=1, add_start_index=True\n",
    "                )\n",
    "\n",
    "print(f\"Text: {doc}\")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "total_chunks = len(chunks)\n",
    "print(f\"Number of chunks: {total_chunks}\")\n",
    "\n",
    "# Generate unique IDs for each chunk\n",
    "\n",
    "# Generate unique IDs for each chunk\n",
    "chunk_ids = [f\"doc_{file_counter}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "\n",
    "batch_size = 128  # Adjust based on your system's capacity\n",
    "total_chunks = len(chunks)\n",
    "\n",
    "\n",
    "#Add chunks data in batches for speed consideration. Operation is same as one chunk at a time in chromadb.\n",
    "#with tqdm(total=total_chunks, desc=f\"{str(file_counter)} / {str(1)} - Adding chunks of {file_path}\", position=1, leave=True, unit=\"chunks\") as pbar:        \n",
    "for batch, batch_ids in create_batches(chunks, chunk_ids, batch_size):\n",
    "    print(\"ilker\")\n",
    "    # Precompute embeddings for the current batch\n",
    "    batch_documents = [chunk.page_content.encode('utf-8', errors='replace').decode('utf-8') for chunk in batch]\n",
    "    batch_metadata = [chunk.metadata for chunk in batch]\n",
    "    batch_embeddings = embedding_function(batch_documents)\n",
    "\n",
    "    #Add the batch to the collection\n",
    "    collection.upsert(\n",
    "        documents=batch_documents,\n",
    "        metadatas=batch_metadata,\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings\n",
    "        )\n",
    "    \n",
    "        # Update the progress bar\n",
    "        #pbar.update(len(batch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client (PersistentClient for persistent storage)\n",
    "client = chromadb.PersistentClient(path=\"D:\\Bilgi\\__Databases\\Bilgi_my_chroma_db_with_langchain\")\n",
    "\n",
    "# Access your collection by name\n",
    "collection = client.get_collection(\"my-rag-db\")\n",
    "\n",
    "total_items = collection.count()\n",
    "print(f\"Total number of element in chromadb: {total_items}\")\n",
    "\n",
    "batch_size = 100000\n",
    "\n",
    "list_items_info = []\n",
    "counter = 0\n",
    "for offset in range(0, total_items, batch_size):\n",
    "    # Retrieve a batch of data\n",
    "\n",
    "    print(f\"Counter: {counter} / {total_items}  \", end=\"\\r\")\n",
    "\n",
    "    batch = collection.get(\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "        limit=batch_size,\n",
    "        offset=offset\n",
    "    )\n",
    "\n",
    "    # Process each item in the batch\n",
    "    for doc, meta, id in zip(batch[\"documents\"], batch[\"metadatas\"], batch[\"ids\"]):\n",
    "        # Replace the following line with your processing logic\n",
    "        #print(f\"Document: {doc}\\nMetadata: {meta}\\nIds: {id}\\n\")\n",
    "\n",
    "        path = meta.get(\"source\", \"Unknown\")\n",
    "        doc_name = path.split(\"\\\\\")[-1]\n",
    "        page = meta.get(\"page\", \"Unknown\")\n",
    "        total_pages = meta.get(\"total_pages\", \"Unknown\")\n",
    "        chunk_length = len(doc)\n",
    "\n",
    "        list_items_info.append([id, chunk_length, path, doc_name, page, total_pages, doc])\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "\n",
    "print(f\"Number of elements: {len(list_items_info)}\")\n",
    "\n",
    "#print(list_items_info)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "directory = \"D:\\\\Bilgi\"\n",
    "file_dict = get_docs(directory)\n",
    "\n",
    "found_docs = []\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for file, info in file_dict.items():\n",
    "    filepath = file_dict[file][0][0]\n",
    "    doc_found = False\n",
    "    for item in list_items_info:\n",
    "        doc_name_in_list = item[2]\n",
    "        if filepath == doc_name_in_list:\n",
    "            doc_found = True\n",
    "            break\n",
    "        #print(filepath , \" - \", doc_name_in_list)\n",
    "    if doc_found:\n",
    "        counter += 1\n",
    "        print(counter, end=\"\\r\")\n",
    "        found_docs.append(filepath)\n",
    "\n",
    "print(f\"Number of found docs: {len(found_docs)}\")\n",
    "\n",
    "\n",
    "found_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: '\n",
      "başlamışlardı ki, tepeden inip de düzlüğe çıktıkları vakit uğursuz \n",
      "bir rüzgar ejderin püskürttüğü alevlerden artakalan yoğun duma­\n",
      "nı onların bulu nduğu tarafa sürükledi ve atların bile aklını ba­\n",
      "şından alacak kadar berbat bir kokuyla çevrelendiler. Sonra, sisin \n",
      "içinde ne yöne gittiklerini göremezken buna bir de ejder nefesinin \n",
      "o iğrenç kokusunun yarattığı panik eklenin ce, atları kont rol altın­\n",
      "da tutmak imkansız hale geldi ve hayvanlar güzergah değiştirip\n",
      "'\n",
      "Document: başlamışlardı ki, tepeden inip de düzlüğe çıktıkları vakit uğursuz \n",
      "bir rüzgar ejderin püskürttüğü alevlerden artakalan yoğun duma­\n",
      "nı onların bulu nduğu tarafa sürükledi ve atların bile aklını ba­\n",
      "şından alacak kadar berbat bir kokuyla çevrelendiler. Sonra, sisin \n",
      "içinde ne yöne gittiklerini göremezken buna bir de ejder nefesinin \n",
      "o iğrenç kokusunun yarattığı panik eklenin ce, atları kont rol altın­\n",
      "da tutmak imkansız hale geldi ve hayvanlar güzergah değiştirip\n",
      "Metadata: {'creationdate': '2015-01-28T21:07:37+02:00', 'creator': 'Adobe Acrobat 11.0', 'moddate': '2015-01-28T21:43:57+02:00', 'producer': 'Adobe Acrobat Pro 11.0.0 Paper Capture Plug-in with ClearScan', 'source': 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\J. R. R. Tolkien - Bitmemiş Öyküler.pdf', 'start_index': 412318, 'title': '', 'total_pages': 756}\n",
      "ID: doc_1792_chunk_1051\n",
      "Similarity Score (Distance): 0.0\n",
      "\n",
      "----------------------------------------------------------\n",
      "Results for query: '\n",
      "başlamışlardı ki, tepeden inip de düzlüğe çıktıkları vakit uğursuz \n",
      "bir rüzgar ejderin püskürttüğü alevlerden artakalan yoğun duma­\n",
      "'\n",
      "Document: başlamışlardı ki, tepeden inip de düzlüğe çıktıkları vakit uğursuz \n",
      "bir rüzgar ejderin püskürttüğü alevlerden artakalan yoğun duma­\n",
      "nı onların bulu nduğu tarafa sürükledi ve atların bile aklını ba­\n",
      "şından alacak kadar berbat bir kokuyla çevrelendiler. Sonra, sisin \n",
      "içinde ne yöne gittiklerini göremezken buna bir de ejder nefesinin \n",
      "o iğrenç kokusunun yarattığı panik eklenin ce, atları kont rol altın­\n",
      "da tutmak imkansız hale geldi ve hayvanlar güzergah değiştirip\n",
      "Metadata: {'creationdate': '2015-01-28T21:07:37+02:00', 'creator': 'Adobe Acrobat 11.0', 'moddate': '2015-01-28T21:43:57+02:00', 'producer': 'Adobe Acrobat Pro 11.0.0 Paper Capture Plug-in with ClearScan', 'source': 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\J. R. R. Tolkien - Bitmemiş Öyküler.pdf', 'start_index': 412318, 'title': '', 'total_pages': 756}\n",
      "ID: doc_1792_chunk_1051\n",
      "Similarity Score (Distance): 0.2292858511209488\n",
      "\n",
      "----------------------------------------------------------\n",
      "Results for query: 'Emevi dönemi ne zaman başlar?'\n",
      "Document: raber hiçbir zaman, ne var olan gerçekleri ne de ele geçirmek istediği ege­\n",
      "menlik konumunu unuttu . İnsan haklar ını Eski Rejim'e karşı kullanmak için, \n",
      "bunlann doğal ve zaman aşımına uğramaz olduklarını ilan ediyordu. Peki \n",
      "bunlar , toplumdan önce gelen, gerçekten de herkese tanınan, egemenliğin \n",
      "bile dokunamayacağı haklar mıydı? Bu konuda, zaman zaman birbiriyle \n",
      "çatışan görüşler ortaya atıldı ve tartışma hiçbir zaman bir sonuca ulaşmadı.\n",
      "Metadata: {'creationdate': '2016-01-27T05:25:15+02:00', 'creator': 'Adobe Acrobat 11.0', 'moddate': '2016-01-27T05:26:57+02:00', 'producer': 'Adobe Acrobat Pro 11.0.0 Paper Capture Plug-in with ClearScan', 'source': 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\Georges Lefebvre Fransız Devrimi.pdf', 'start_index': 388988, 'title': '', 'total_pages': 665}\n",
      "ID: doc_1782_chunk_997\n",
      "Similarity Score (Distance): 0.5146365165710449\n",
      "\n",
      "----------------------------------------------------------\n",
      "Result \n",
      "\n",
      "Chunk id: doc_1782_chunk_997, metadata: E:\\__ilker\\Bilgi\\Kitaplar\\Çeşitli\\Georges Lefebvre Fransız Devrimi.pdf \n",
      "Content: raber hiçbir zaman, ne var olan gerçekleri ne de ele geçirmek istediği ege­\n",
      "menlik konumunu unuttu . İnsan haklar ını Eski Rejim'e karşı kullanmak için, \n",
      "bunlann doğal ve zaman aşımına uğramaz olduklarını ilan ediyordu. Peki \n",
      "bunlar , toplumdan önce gelen, gerçekten de herkese tanınan, egemenliğin \n",
      "bile dokunamayacağı haklar mıydı? Bu konuda, zaman zaman birbiriyle \n",
      "çatışan görüşler ortaya atıldı ve tartışma hiçbir zaman bir sonuca ulaşmadı.\n",
      "[Document(id='doc_1782_chunk_997', metadata={'creationdate': '2016-01-27T05:25:15+02:00', 'creator': 'Adobe Acrobat 11.0', 'moddate': '2016-01-27T05:26:57+02:00', 'producer': 'Adobe Acrobat Pro 11.0.0 Paper Capture Plug-in with ClearScan', 'source': 'E:\\\\__ilker\\\\Bilgi\\\\Kitaplar\\\\Çeşitli\\\\Georges Lefebvre Fransız Devrimi.pdf', 'start_index': 388988, 'title': '', 'total_pages': 665}, page_content=\"raber hiçbir zaman, ne var olan gerçekleri ne de ele geçirmek istediği ege\\xad\\nmenlik konumunu unuttu . İnsan haklar ını Eski Rejim'e karşı kullanmak için, \\nbunlann doğal ve zaman aşımına uğramaz olduklarını ilan ediyordu. Peki \\nbunlar , toplumdan önce gelen, gerçekten de herkese tanınan, egemenliğin \\nbile dokunamayacağı haklar mıydı? Bu konuda, zaman zaman birbiriyle \\nçatışan görüşler ortaya atıldı ve tartışma hiçbir zaman bir sonuca ulaşmadı.\")]\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Only find similar document chunks according to queries -- for testing\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "\n",
    "# Initialize the Chroma client (PersistentClient for persistent storage)\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# embedding_function = OllamaEmbeddingFunction(\n",
    "#     model_name=\"nomic-embed-text\",\n",
    "#     url=\"http://localhost:11434/api/embeddings\",\n",
    "# )\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name = \"my-rag-db\",\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"D:\\\\Bilgi\\\\__Databases\\\\Bilgi_my_chroma_db_with_langchain\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define your queries\n",
    "queries = [\n",
    "\"\"\"\n",
    "başlamışlardı ki, tepeden inip de düzlüğe çıktıkları vakit uğursuz \n",
    "bir rüzgar ejderin püskürttüğü alevlerden artakalan yoğun duma­\n",
    "nı onların bulu nduğu tarafa sürükledi ve atların bile aklını ba­\n",
    "şından alacak kadar berbat bir kokuyla çevrelendiler. Sonra, sisin \n",
    "içinde ne yöne gittiklerini göremezken buna bir de ejder nefesinin \n",
    "o iğrenç kokusunun yarattığı panik eklenin ce, atları kont rol altın­\n",
    "da tutmak imkansız hale geldi ve hayvanlar güzergah değiştirip\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "başlamışlardı ki, tepeden inip de düzlüğe çıktıkları vakit uğursuz \n",
    "bir rüzgar ejderin püskürttüğü alevlerden artakalan yoğun duma­\n",
    "\"\"\",\n",
    "\"Emevi dönemi ne zaman başlar?\"   \n",
    "]\n",
    "\n",
    " \n",
    "# Iterate over each query and perform similarity search with scores\n",
    "for query in queries:\n",
    "    results = await vector_store.asimilarity_search_with_score(query, k=1)  # Adjust 'k' as needed\n",
    "\n",
    "    # Process and display results\n",
    "    print(f\"Results for query: '{query}'\")\n",
    "    for doc, score in results:\n",
    "        print(f\"Document: {doc.page_content}\")  # Display first 100 characters\n",
    "        print(f\"Metadata: {doc.metadata}\")  # Display first 100 characters\n",
    "        print(f\"ID: {doc.id}\")  # Display first 100 characters\n",
    "        print(f\"Similarity Score (Distance): {score}\\n\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#Create a retriever from vectorstore\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={ \"k\": 1, \"lambda_mult\": 0.7,  \"score_threshold\": 0.8, \"fetch_k\": 10}\n",
    ")\n",
    "found_chunks = retriever.invoke(queries[2])\n",
    "\n",
    "for i in range(len(found_chunks)):\n",
    "    print(\"Result \\n\")\n",
    "    print(f\"Chunk id: {found_chunks[i].id}, metadata: {found_chunks[i].metadata['source']} \")\n",
    "    print(f\"Content: {found_chunks[i].page_content}\")\n",
    "print(found_chunks)\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "#---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created...\n",
      "bm25_data_docs_and_metadata.json read...\n",
      "bm25_retriever ready...\n",
      "data is deleted from RAM...\n"
     ]
    }
   ],
   "source": [
    "#Full RAG app with hybrid search 1/2 - preparation of retrievers \n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from pydantic import Field\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import json\n",
    "import msgspec\n",
    "from langchain.schema import Document\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Dict, Optional\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "class Data(msgspec.Struct):\n",
    "    documents: list[str]\n",
    "    metadatas: list[dict]\n",
    "\n",
    "\n",
    "#Functions for Post-processing to create list of reference documents the LLM used from the all reference documents (text chunks + metadata)\n",
    "def keyword_matching(llm_output, documents):\n",
    "    matched_docs = []\n",
    "    for doc in documents:\n",
    "        doc_keywords = extract_keywords(doc)\n",
    "        if any(keyword in llm_output for keyword in doc_keywords):\n",
    "            matched_docs.append(doc)\n",
    "    return matched_docs\n",
    "\n",
    "def extract_keywords(document):\n",
    "    # Implement your keyword extraction logic here\n",
    "    # This could be as simple as splitting the document into words\n",
    "    # or using more advanced techniques like TF-IDF or RAKE\n",
    "    return document.split()\n",
    "\n",
    "def similarity_assessment(llm_output, documents, threshold=0.5):\n",
    "    # Ensure documents is a list of strings\n",
    "    documents = list(documents)  # Convert generator to list if necessary\n",
    "    documents = [str(doc) for doc in documents]  # Ensure each document is a string\n",
    "\n",
    "    # Combine LLM output and documents into a single corpus\n",
    "    corpus = [llm_output] + documents\n",
    "\n",
    "    # Compute TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer().fit_transform(corpus)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Extract LLM output vector and document vectors\n",
    "    llm_vector = vectors[0]\n",
    "    doc_vectors = vectors[1:]\n",
    "\n",
    "    # Compute cosine similarity between LLM output and each document\n",
    "    similarities = cosine_similarity([llm_vector], doc_vectors).flatten()\n",
    "\n",
    "    # Filter documents based on similarity threshold\n",
    "    matched_docs = [doc for doc, sim in zip(documents, similarities) if sim >= threshold]\n",
    "\n",
    "    return matched_docs\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#Custom BM25Retriever in order to utilize if all keywords are inside a text chunk, give higher score to that chunk in ranking with scores\n",
    "class CustomBM25Retriever(BaseRetriever):\n",
    "    k1: float = Field(default=1.2)\n",
    "    b: float = Field(default=0.75)\n",
    "    phrase_boost: float = Field(default=1.5)\n",
    "    k: int = Field(default=10)  # Number of top documents to retrieve\n",
    "    documents: List[Document] = Field(default_factory=list)\n",
    "    tokenized_docs: List[List[str]] = Field(default_factory=list)\n",
    "    bm25: Optional[BM25Okapi] = None\n",
    "\n",
    "    def __init__(self, documents: List[Document], k: int = 10, k1: float = 1.2, b: float = 0.75, phrase_boost: float = 1.5):\n",
    "        super().__init__(documents=documents, k=k, k1=k1, b=b, phrase_boost=phrase_boost)\n",
    "        self.k = k\n",
    "        self.tokenized_docs = [self.tokenize(doc) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs, k1=self.k1, b=self.b)\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(cls, texts: List[str], metadatas: Optional[List[Dict]] = None, k: int = 10, bm25_params: Optional[Dict] = None):\n",
    "        \"\"\" Factory method to initialize from raw texts and metadata (similar to LangChain's BM25Retriever) \"\"\"\n",
    "        bm25_params = bm25_params or {\"k1\": 1.2, \"b\": 0.75}\n",
    "        documents = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadatas or [{}] * len(texts))]\n",
    "        return cls(documents, k=k , **bm25_params)\n",
    "\n",
    "    def tokenize(self, doc: Document):\n",
    "        \"\"\" Tokenizes both content and metadata for retrieval \"\"\"\n",
    "        metadata_str = \" \".join(f\"{key}: {value}\" for key, value in doc.metadata.items())\n",
    "        full_text = f\"{doc.page_content} {metadata_str}\"\n",
    "        return full_text.split()\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\" Retrieve and rank documents using BM25 with metadata and phrase boosting \"\"\"\n",
    "        query_tokens = query.split()\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "\n",
    "        # Boost score if query appears as a phrase\n",
    "        boosted_scores = []\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            full_text = f\"{doc.page_content} \" + \" \".join(f\"{key}: {value}\" for key, value in doc.metadata.items())\n",
    "            phrase_bonus = self.phrase_boost if query in full_text else 1.0\n",
    "            boosted_scores.append(scores[i] * phrase_bonus)\n",
    "\n",
    "        # Rank documents by boosted BM25 score\n",
    "        ranked_docs = sorted(zip(self.documents, boosted_scores), key=lambda x: x[1], reverse=True)\n",
    "        return [doc[0] for doc in ranked_docs[:self.k]]  # Return only top_k documents\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# Function to process retrieved documents\n",
    "class RetrieveAndFormat(Runnable):\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs: Dict[str, str], config=None) -> Dict[str, str]:\n",
    "        question = inputs['question']\n",
    "        retrieved_docs: List[Document] = self.retriever.invoke(question)\n",
    "        if not retrieved_docs:\n",
    "            return {\"context\": \"No info\", \"metadata\": \"No info\", \"question\": question, \"context_with_metadata\": \"No info\"}\n",
    "        \n",
    "        reordering = LongContextReorder()\n",
    "        reordered_docs = reordering.transform_documents(retrieved_docs)\n",
    "\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in reordered_docs])\n",
    "\n",
    "        # Initialize an empty set to track seen sources\n",
    "        seen_sources = set()\n",
    "\n",
    "        # Initialize a list to store unique metadata entries\n",
    "        unique_metadata = []\n",
    "\n",
    "        # Iterate over the retrieved documents\n",
    "        counter=1\n",
    "        for doc in reordered_docs:\n",
    "            # Extract the 'source' metadata, defaulting to 'Unknown' if not present\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            # If this source has not been encountered before, add it to the set and list\n",
    "            if source not in seen_sources:\n",
    "                seen_sources.add(source)\n",
    "                unique_metadata.append(f\"Source-{str(counter)}: {source}\")\n",
    "                counter += 1\n",
    "\n",
    "        # Join the unique metadata entries into a single string with double newlines\n",
    "        metadata = \"\\n\\n\".join(unique_metadata)\n",
    "\n",
    "        context_with_metadata = \"\\n\\n\".join([\"\\n\".join([doc.page_content, doc.metadata.get('source', 'Unknown')]) for doc in reordered_docs])\n",
    "\n",
    "        return {\"context\": context, \"metadata\": metadata, \"question\": question, \"context_with_metadata\": context_with_metadata}\n",
    "    \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# Initialize the Chroma client (PersistentClient for persistent storage)\n",
    "#embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embedding = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name = \"my-rag-db\",\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"D:\\\\Bilgi\\\\__Databases\\\\Bilgi_my_chroma_db_with_langchain\"\n",
    "    )\n",
    "\n",
    "print(\"Vectorstore created...\")\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# #Create bm25 retriever and save it for one time\n",
    "# stored_data = vector_store.get()\n",
    "# doc_list =  stored_data.get('documents', [])\n",
    "# metadata_list = stored_data.get('metadatas', [])\n",
    "# data_to_save = {\n",
    "#     'documents': doc_list,\n",
    "#     'metadatas': metadata_list\n",
    "# }\n",
    "\n",
    "# print(\"Data to save created for BM25 retriever...\")\n",
    "\n",
    "# with open('D:\\\\Bilgi\\\\__Databases\\\\bm25_data_docs_and_metadata.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(data_to_save, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(\"Data to saved for BM25 retriever...\")\n",
    "\n",
    "#------------------------\n",
    "#Read the document chunks and metadata as a file and put them into the BM25REtriever -- heavy RAM load\n",
    "with open('D:\\\\Bilgi\\\\__Databases\\\\bm25_data_docs_and_metadata.json', 'rb') as file:\n",
    "    data = msgspec.json.decode(file.read(), type=Data)\n",
    "\n",
    "print(\"bm25_data_docs_and_metadata.json read...\")\n",
    "\n",
    "# Initialize Custom BM25 Retriever instead of default BM25Retriever\n",
    "#bm25_retriever = BM25Retriever.from_texts(data.documents, metadatas=data.metadatas, bm25_params={\"k1\": 0.8, \"b\": 0.5})\n",
    "bm25_retriever = CustomBM25Retriever.from_texts(data.documents, data.metadatas, k=10, bm25_params={\"k1\": 0.8, \"b\": 0.5, \"phrase_boost\": 1.0})\n",
    "\n",
    "print(\"bm25_retriever ready...\") \n",
    "\n",
    "del data\n",
    "gc.collect()\n",
    "print(\"data is deleted from RAM...\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble_retriever created\n",
      "RAG chain ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Zaman Tüneli ile Geçmişte Sümer!e Yolculuk Kitabında Sümerli Çocuğun Ağızdan Ne Anlatmış?', \"Sümer!e Yolculuk Kitabı'nda Zaman Tüneli ile İlgili Sümerli Çocuk Anlatımları Nelerdir?\", 'Zaman Tüneli Olayları Hakkında Sümerli Çocuk Anlatımının Detayları Nelerdir?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "----------------------\n",
      "Answer: \n",
      "Zaman Tüneli ile Geçmişte Sümer'e Yolculuk kitabında Sümerli çocuk Lu Dingirra, \"Tanrının adamı\" anlamına gelir ve kısaca \"Lu\" diye de söylenebilir. 14 yaşında olup, 6 yıldır okula gidiyor ve en az 5 yıl daha okuması gerektiğini söylüyor. Kara saçlı kara gözlü bir Sümerli çocuğu anlatıyor ve kendisinin ülkesi ve yaşamını tanıtmak için zaman tüneli yolculuğuna davet ediyor.\n",
      "\n",
      "Relevant documents LLM model used:\n",
      "\n",
      "Source-1: D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n",
      "\n",
      "\n",
      "----------------------\n",
      "----------------------\n",
      "All relevant documents:\n",
      "\n",
      "Source-1: D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n",
      "\n",
      "Source-2: D:\\Bilgi\\Kitaplar\\Çeşitli\\Reşat Nuri Güntekin - Çalıkuşu.pdf\n",
      "\n",
      "Source-3: D:\\Bilgi\\Kitaplar\\Çeşitli\\Doğan Cüceloğlu - Onlar Benim Kahramanım.pdf\n",
      "\n",
      "Source-4: D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ - İbrahim Peygamber.pdf\n",
      "\n",
      "Source-5: D:\\Bilgi\\KOSGEB_Girişimcilik_El_Kitabi-2-mart-2020.pdf\n",
      "\n",
      "Source-6: D:\\Bilgi\\Kitaplar\\Çeşitli\\Stephen Hawking - Kara Delikler.pdf\n",
      "\n",
      "Source-7: D:\\Bilgi\\Kitaplar\\Çeşitli\\Üstün Dökmen - Yaşama Yerleşmek.pdf\n",
      "\n",
      "Source-8: D:\\Bilgi\\Kitaplar\\Çeşitli\\Evrenin Dokusu - Brian Greene ( PDFDrive.com ).pdf\n",
      "\n",
      "Source-9: D:\\Bilgi\\Kitaplar\\Information Theory\\Math - Information Theory, Inference, and Learning Algorithms.pdf\n",
      "\n",
      "Source-10: D:\\Bilgi\\Kitaplar\\Çeşitli\\J. R. R. Tolkien - Bitmemiş Öyküler.pdf\n",
      "\n",
      "Source-11: D:\\Bilgi\\Kitaplar\\Çeşitli\\Carl Gustav Jung - Dörtarketip.pdf\n",
      "\n",
      "Source-12: D:\\Bilgi\\Kitaplar\\Çeşitli\\Gösteri Toplumu - Guy Debord ( PDFDrive.com ).pdf\n",
      "\n",
      "Source-13: D:\\Bilgi\\Kitaplar\\Çeşitli\\Hamlin Çocukları - Carmen Carter ( PDFDrive.com ).pdf\n",
      "\n",
      "\n",
      "----------------------\n",
      "----------------------\n",
      "Zaman Tüneli İle Geçmişte\n",
      "Sumer'e Yolculuk\n",
      "Muazzez İlmiye ÇIĞ\n",
      "KÜLTÜR BAKANLIĞI\n",
      "ÇOCUK/EDEBİYAT\n",
      "\fKÜLTÜR BAKANLIĞI YAYINLARI / 1520\n",
      "Yayımlar Dairesi Başkanlığı\n",
      "Çocuk - Edebiyat - Dizisi / 134 - 6\n",
      "ZAMAN TÜNELİ İLE\n",
      "GEÇMİŞTE SÜMER'E\n",
      "YOLCULUK\n",
      "Yazan: Resimleyen:\n",
      "Muazzez İlmiye ÇIĞ İbrahim TAPA\n",
      "\f© Kültür Bakalığı / 1993 ANKARA\n",
      "ISBN 975-17-1306-4\n",
      "Yayımlar Dairesi Başkanlığı'nın 21.5.1993 tarih ve\n",
      "928.1.1299 sayılı makam onayı gereğince ilk defa olarak\n",
      "10.000 adet bastırılmıştır.\n",
      "Hassoy Matbaası\n",
      "ANKARA\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n",
      "\n",
      "başlattığımız girişimlerimizi 1993'te yine gündeme getiriyor\n",
      "\"KİTAP SATIN ALIN... AYDINLANIN\" diyoruz.\n",
      ". Çoğulcu, katılımcı, demokratik yapılı, çağını yakalamış,\n",
      "ona içerden bakan kuşaklar olarak yetişmemizin gerçeği bu di-\n",
      "yorum ve buna içtenlikle inanıyorum.\n",
      "D. Fikri SAĞLAR\n",
      "Kültür Bakanı\n",
      "VI\n",
      "\fÖNSÖZ\n",
      "İlkokul ve ortaokul çocukları için yazdığım bu\n",
      "küçük kitapta amacım, bundan 4000 yıl önce\n",
      "yaşamış olan yaşdaşları ve hemen hemen aynı\n",
      "ruhu taşıyan bir Sumerli çocuğun ağzından, Sümer\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n",
      "\n",
      "Sesim kesilir kesilmez onun başlayacağına hiç şüphem yok. Ne yapmalı? Onunla yüz yüze gelmeden\n",
      "nasıl kaçmalı?\n",
      "Eteklerime bir çocuğun sarıldığını görüyorum, koltuklarının altından tutarak havaya kaldırıyorum.\n",
      "Bu, misafirlerimin en miniminisi, yedi, sekiz yaşında bir bebekti. Yüzünü yüzüme yaklaştırarak:\n",
      "-Hatırın kalmasın ama, seninle hiç olmaz diyorum. Bu tombul yanacıkları kanatırsak ne olur?\n",
      "Çocuğun arkasını bir gölge kaplıyor. Bu, Kâmran’dır. Bu başı başımdan ayırır ayırmaz onunla yüz\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Reşat Nuri Güntekin - Çalıkuşu.pdf\n",
      "\n",
      "miş gibi. Böyle gerine gerine giderdik, hatırlıyorum kurumlanma­\n",
      "yı böyle. Acayip bir duygu.\n",
      "Gültekin Bey sesli sesli gülüyor.\n",
      "T: Otobüslerde giderdik, alırdık kucağımıza çocuğumuzu oto­\n",
      "büste. Etrafına bakardı, iletişim kurardı. Hemen kaş göz yapardı \n",
      "mesela küçücükken... İzmir'e geldiğimizde bir yaşındaydı. Daha \n",
      "yürümüyordu. 13 aylık yürüdü amcasının evinde. 13 aylıkken de \n",
      "ilk defa ağzından \"yenye\" sözü çıktı yengeye. İlk konuşmaları öy­\n",
      "le oldu.\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Doğan Cüceloğlu - Onlar Benim Kahramanım.pdf\n",
      "\n",
      "lar evlenmişler. \n",
      "16 C.L. Vı'oolley, Tlıe Sumerians. Nc\\v York. 1965. s.21: S.N. Kramer, The Simle-\n",
      "rimi*. s.328.\n",
      "\fmı, rahatlık  veya  dinlence)  bildirerek  bir  gemi  yapmasını;  içine  ailesini, \n",
      "bütün  hayvanlardan,  ikişer,  yedişer  almasını  söylüyor.  Tufan  40  gün  sü-\n",
      "rüyor.  Gemi  Ararat  Dağı'na  yanaşıyor.  150  gün  sonra  gemiden  çıkılıyor. \n",
      "Rab  yaptığına  pişman  oluyor  ve  Nuh'a  950  yıl  ömür  veriyor. \n",
      "Bu hikâye  de  Sumerlilerden  kaynaklanıyor.  Sümer  anlatılışına  göre,\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ - İbrahim Peygamber.pdf\n",
      "\n",
      "göre (2013) bir girişimcinin profesyonel çalışana kıyasla %63 daha fazla çalıştığı da araştırmalarla \n",
      "kanıtlanmıştır. İnsanın kendi işini yapması önemli bir değerdir. Ancak bir girişimcinin ne kadar \n",
      "çalışması gerektiği düşünüldüğünde bunun zorlu bir yolculuk olduğu söylenebilir. İşte bu sebeple \n",
      "girişimci planlı, akıllı ve bol bağlantılı olmalıdır.\n",
      "Özetle networking, girişimcinin tanınmasını ve doğru insanlara ulaşmasını hızlandırır. Zaman ve\n",
      "D:\\Bilgi\\KOSGEB_Girişimcilik_El_Kitabi-2-mart-2020.pdf\n",
      "\n",
      "delikten geçerek arkadaki dedektör ekranında \n",
      "saptanıyorlar. Bu fotonlarm bir kısmı metale \n",
      "çarparak geri gelirler, bir kısmı geçer. Hangi­\n",
      "sinin geçeceği tamamen belirsizdir. Ancak de­\n",
      "tektöre çarptıktan sonra bu konuda bilgimiz \n",
      "olabilir. Şimdi dedektör ekranını kaldırdığımı-\n",
      "Bu noktada, Roger Penrose'un bilgi kaybı tartışmasına çok \n",
      "farklı bir yaklaşım getirdiğini vurgulayalım. Zaman Döngü­\n",
      "leri kitabında anlattığı bu yaklaşım, Havvking ile Susskind\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Stephen Hawking - Kara Delikler.pdf\n",
      "\n",
      "mayan, orkestranın bir parçası olarak algılanan çalgılar hali­\n",
      "ne gelebilir. Tıpkı, geçmişte çeşitli restorasyonlar ve renovas-\n",
      "\f13 6 YAŞAMA YERLEŞMEK \n",
      "yanlar geçirmiş olan mimari yapıların artık tüm eski değişik­\n",
      "lerle birlikte bir bütün olarak algılanıyor olmaları gibi. ) \n",
      "İnsan İlişkilerinde Restorasyon \n",
      "Mimaridekine, moda tasarımındakine, müziktekine ben­\n",
      "zer şekilde insan ilişkilerinde de restorasyon gerekebilir. \n",
      "Zaman zaman farkında olmadan yaptığımız restorasyonları\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Üstün Dökmen - Yaşama Yerleşmek.pdf\n",
      "\n",
      "iradeyi\tgöz\tönüne\talalım.\n",
      "Eğer\tözgür\tirade\tbir\tyanılsama\tise\tve\tgeçmiş\n",
      "zamana\t yolculuk\t yapmak\t olası\t ise,\t o\t zaman\n",
      "anne\t ve\t babanızın\t karşılaşmasını\n",
      "önleyememeniz\tbir\tbilmece\toluşturmaz.\tHer\tne\n",
      "kadar\t sanki\t eylemleriniz\t üzerinde\t bir\n",
      "kontrolünüz\tvarmış\tgibi\thissediyor\tolsanız\tda,\n",
      "aslında\ther\tşeyi\tyapan\tfizik\tyasalarıdır.\tAnnenizi\n",
      "oradan\tuzaklaştırmak\tveya\tbabanızı\tvurmak\tiçin\n",
      "oraya\tgittiğinizde\tfizik\tyasaları\toradadır.\tZaman\n",
      "makinesi\t sizi\t şehrin\t yanlış\t tarafına\t indirir,\t bu\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Evrenin Dokusu - Brian Greene ( PDFDrive.com ).pdf\n",
      "\n",
      "\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\n",
      "\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\n",
      "\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\n",
      "\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\u0001\u000b\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\u0001\n",
      "D:\\Bilgi\\Kitaplar\\Information Theory\\Math - Information Theory, Inference, and Learning Algorithms.pdf\n",
      "\n",
      "\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\u0001\f\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "\u0001\n",
      "D:\\Bilgi\\Kitaplar\\Information Theory\\Math - Information Theory, Inference, and Learning Algorithms.pdf\n",
      "\n",
      "\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\n",
      "\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\n",
      "\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\n",
      "\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\u0001\u001b\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\u0001\u001c\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\n",
      "\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\u001d\u0001\n",
      "D:\\Bilgi\\Kitaplar\\Information Theory\\Math - Information Theory, Inference, and Learning Algorithms.pdf\n",
      "\n",
      "\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\n",
      "\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\t\u0001\n",
      "D:\\Bilgi\\Kitaplar\\Information Theory\\Math - Information Theory, Inference, and Learning Algorithms.pdf\n",
      "\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\u0001\u0015\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\u0001\u0016\n",
      "D:\\Bilgi\\Kitaplar\\Information Theory\\Math - Information Theory, Inference, and Learning Algorithms.pdf\n",
      "\n",
      "bile, yine de içeri girmeyi başaramazsın .\" \n",
      "\"Beni o kapıların ötesine götürmeni beklemiyo rum,\" dedi \n",
      "Tuor. \"Or aya vardığım an, kötülüğün gücü Ulmo'nun irade gü­\n",
      "cüyle çat ışacak ve sonuç öyle ortaya çıkacak . Turgon beni huzuru­\n",
      "na kabul etmezse, görevim orada sonianmış olacak ve dolayısıyla \n",
      "kötülüğün iradesi galip gelec ek. Turgon'u arama hakkını kendim­\n",
      "de görmeme gelinc e: Ben Huor'un oğlu Tuor'um ve bu bağlamda \n",
      "Hurin ile de akrabayım; bu saydıklarım, Turgon'un asla unut a­\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\J. R. R. Tolkien - Bitmemiş Öyküler.pdf\n",
      "\n",
      "terin dibindedir ve etrafı yüksek kayalarla çevrilidir, bir başka sefer \n",
      "de miniciktir ve alçak, taşlı bir alanda yükselen bir dağın tepesinde­\n",
      "dir. Aynı motif Goethe'nin, evi küçük bir kutunun içine sığan cüce \n",
      "prenses masalında da vardır.31 Bu bağlamda Anthroparion'u, Zosi­\n",
      "mos'un vizyonundaki 32 Kurşunadamcık'ı, maden ocaklarının Demi­\n",
      "radamcık'ını, antikçağın hünerli Daktyller'ini, simyacıların homuncu­\n",
      "* Sepette ne olduğunu soran küçük bir demir adam (ç.n.)\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Carl Gustav Jung - Dörtarketip.pdf\n",
      "\n",
      "onların  yazı  ve  dillerini  öğrenerek  Babil  kitaplıklarını  incelemişler.  Bu \n",
      "yüzden  özellikle  Tekvinin  ilk  ikinci  bölümüne  kadar  olan  konular \n",
      "100 Meissner,  Babylcnien und  Assyien,  Band  I,  Heidelbeıg,  1925,  s.281.\n",
      "\ftamamıyla  Sümer  efsanelerinden  alınmış.  Diğer  kısımlarda  da  alıntıları \n",
      "gösterdik.  Bu  beş  kitaptan  sonraki  konular  da,  zaman  zaman  yazılan  me-\n",
      "tinlerin  bir  araya  getirilmesi  ile  oluşmuş.  Bunların  hemen  hepsi  tarihsel\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ - İbrahim Peygamber.pdf\n",
      "\n",
      "bağlantı  yok.  Allah'ın  gemi  ile  onları  kurtardığı  ve  Nuh'a  950  yıl  ömür \n",
      "17 Tarilı  Sümer'de  Başlar,  s.  128-132;  S  N.  Kramer,  İn  tlıe  Wurld  of  Sümer,  an  Au-\n",
      "robiography, Detroit,  1986,  s.99. \n",
      "18 A'râf  Suresi,  ayet  59;  Yunus  Suresi,  ayet  73;  Hûd  Suresi,  ayet  36-44;  Mü'min  Suresi, \n",
      "ayet 26-29;  Şûrâ  Suresi,  ayet  117-120;  Ankebût  Suresi,  ayet  14-15;  Zâriyât  Suresi,\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ - İbrahim Peygamber.pdf\n",
      "\n",
      "dünyanın\t fazlalıklarını\t tanımlar.\tAma,\t imajların\n",
      "yüzeysel\t hâkimiyetini,\t psikolojik\t ve\t manevî\n",
      "değerlendirmelerle,\t “bizim\t zırva\t isteklerimizin”\n",
      "ürünü\t olarak\t nitelerken\t üstü\t kapalı\t olarak\n",
      "başvurduğu\t toplumsal\t yaşamın\t “normal\n",
      "temeli”nin\t ne\t Boorstin’in\t kitabında\t ne\t de\n",
      "döneminde\t hiçbir\t gerçekliği\t yoktur.\t Boorstin,\n",
      "bir\t imaj\t toplumunu\t derinlemesine\t anlayamaz,\n",
      "çünkü\t sözünü\t ettiği\t gerçek\t insan\t yaşamı,\t ona\n",
      "göre,\tdinsel\ttevekkülü\tde\tkapsayan\tbir\tgeçmişte\n",
      "kalmıştır.\t Bu\t toplumun\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Gösteri Toplumu - Guy Debord ( PDFDrive.com ).pdf\n",
      "\n",
      "yaşamış olan yaşdaşları ve hemen hemen aynı\n",
      "ruhu taşıyan bir Sumerli çocuğun ağzından, Sümer\n",
      "yaşantısını ve kültürünü onlara eğlenceli bir şekilde\n",
      "tanıtmaktır.\n",
      "Büyük Atatürk, dilleri dilimize çok benzeyen ve\n",
      "Türklerin anayurdu Asya topraklarından göç ettikleri-\n",
      "ne inandığı (bugün bu doğrulanmaktadır) Sumerlile-\n",
      "rin ve kültürlerinin ülkemizde tanınmasını ve bilin-\n",
      "mesini istiyordu. Ne yazık ki, bugüne kadar bu\n",
      "yönde herkesin ilgisini çekecek bir çalışma yapıla-\n",
      "madı.\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n",
      "\n",
      "noktalarına\tyolculuk\tyapabilecek\tbir\n",
      "araca\tdönüştürülmüştü.\tMürettebatı,\n",
      "kendilerini\tönlerinde\tbaştan\tçıkarıcı\tbir\n",
      "şekilde\tuzanan\tyeni\tyerleri\tkeşfetmeye\n",
      "adamış\ten\tkalifiye\tsubay\tve\tbilim\n",
      "adamlarından\toluşmaktaydı.\n",
      "Zaman\tzaman\taraştırma\tölümcül\n",
      "olabilirdi.\n",
      "Atılgan\tkalkanları\taçılmış\tve\tsilahları\n",
      "hazır\tşekilde,\tbaş\tdöndürücü\tbir\tışık\tseli\n",
      "içerisinde\tWarp\thızından\tdüşerek\tsavaş\n",
      "alanına\tyaklaştı.\n",
      "\"Bay\tData,\tbu\tmavi\tşey\tile\tilgili\tne\n",
      "söyleyebilirsiniz?\"\tdiye\tsordu\tkaptan,\n",
      "USS\tFerrel’in\tve\tona\tsaldırmakta\tolan\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Hamlin Çocukları - Carmen Carter ( PDFDrive.com ).pdf\n",
      "\n",
      "Adım \"Ludingirra\", anlamı \"Tanrının adamı\". Adımı\n",
      "söylemek size zor gelirse, kısaca \"Lu\" diyebilirsiniz.\n",
      "Tam 14 yaşındayım. 6 yıldan beri okula gidiyorum.\n",
      "Eğer okulda her bilgiyi öğrenmek istersem, daha en\n",
      "az 5 yıl okumam gerek. Boyum çok uzun değil, kara\n",
      "saçlı kara gözlüyüm ama derim kara değil. Zaten\n",
      "bizim halkımız hep kara gözlü, kara saçlı. Herhalde\n",
      "onun için biz kendimize \"karabaşlı\" diyoruz.\n",
      "Ben şimdi, yaşantımı, ülkemi tanıtmak için sizi\n",
      "zaman tüneli ile geçmişe götüreceğim. Hoşunuza gi-\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n",
      "\n",
      "farklı bir yaklaşım getirdiğini vurgulayalım. Zaman Döngü­\n",
      "leri kitabında anlattığı bu yaklaşım, Havvking ile Susskind \n",
      "arasındaki tartışmadan çok farklı olduğu için ayrıntılarına \n",
      "burada girmeyeceğiz.\n",
      "27\n",
      "\fKARA DELİKLER\n",
      "zı düşünelim. Bu durumda hiç bir şekilde han­\n",
      "gi fotonun delikten geçtiğini ve ne tarafa doğru \n",
      "gittiğini bilemeyiz, dolayısıyla belli bir zaman \n",
      "diliminden sonra geriye doğru giderek fotonun \n",
      "daha önce nerede olduğunu bilemeyiz.\n",
      "ışık kaynağı\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Stephen Hawking - Kara Delikler.pdf\n",
      "\n",
      "Ben şimdi, yaşantımı, ülkemi tanıtmak için sizi\n",
      "zaman tüneli ile geçmişe götüreceğim. Hoşunuza gi-\n",
      "deceğini umut ediyorum. Geçmiş zaman deyince\n",
      "aklınıza ne gelir? Dün, geçen hafta, geçen ay, ge-\n",
      "çen yıl, geçen yüzyıl hepsi geçmiş zaman değil mi?\n",
      "Ama bizim yolculuğumuz çok daha eskilere uzaya-\n",
      "cak, hazır mısınız?\n",
      "Yolculuğumuza içinde bulunduğumuz 20. yüz-\n",
      "yıldan çıkıyoruz. Bu yolculuğumuzda aynı topraklar-\n",
      "da birbirini ardınca yaşayan milletler, değişik şehir-\n",
      "D:\\Bilgi\\Kitaplar\\Çeşitli\\Muazzez İlmiye Çığ -  Zaman Tüneli ile Geçmişte Sümer'e Yolculuk.pdf\n"
     ]
    }
   ],
   "source": [
    "#Full RAG app with hybrid search 2/2 - applying doc search with question\n",
    "\n",
    "question = \"Zaman tüneli ile geçmişte sümer!e yolculuk kitabında sümerli çocuğun ağzından ne anlatmış?\"   \n",
    "\n",
    "\n",
    "model = \"mistral-nemo\"  #\"llama3.2:1b\"\n",
    "llm = ChatOllama(model=model, temperature=0.1, repeat_penalty=1.1, streaming=True)  #, callbacks=[StreamingStdOutCallbackHandler()]\n",
    "\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "input_variables=[\"question\"],\n",
    "template=\n",
    "\"\"\"You are an AI language model assistant. Your task is taking a natural language query from a user and converting it into \n",
    "three queries for a vectorstore with the same language of the question. By generating multiple perspectives on the user question, \n",
    "your goal is to help the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_PROMPT = PromptTemplate(\n",
    "input_variables=[\"question\"],\n",
    "template=\n",
    "\"\"\"You are an assistant tasked with taking a natural language\n",
    "query from a user and converting it into a query for a vectorstore with the same language of the question.\n",
    "In this process, you strip out information that is not relevant for\n",
    "the retrieval task. Here is the user query: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "#Alternative parameters for search_type=\"similarity\" or \"mmr\"\n",
    "#search_as = {\"score_threshold\": 0.6, \"k\": 10}  #For search_type=\"similarity\"\n",
    "search_as = { \"k\": 5, \"lambda_mult\": 0.8,  \"score_threshold\": 0.6, \"fetch_k\": 20} #'filter': {'paper_title':'GPT-4 Technical Report'}\n",
    "retriever_contextual = MultiQueryRetriever.from_llm(\n",
    "    #vector_store.as_retriever(search_type=\"similarity_score_threshold\",search_kwargs=search_as), \n",
    "    vector_store.as_retriever(search_type=\"mmr\",search_kwargs=search_as), \n",
    "    llm=llm, \n",
    "    prompt=QUERY_PROMPT   #DEFAULT_PROMPT,\n",
    ")\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "#Hybrid retrieval: This is for extracting relevant text chunks and metadata according to the question. They will be sent to LLM for creating answer\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever_contextual], rank_fusion=\"reciprocal\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"ensemble_retriever created\")\n",
    "\n",
    "# RAG prompt: It contains retrieved text chunks and metadata, also chunks+metadata together\n",
    "template = \"\"\"Answer the question using only most relevant context-with-metadata among the provided ones with the same language of the question.\n",
    "Generate concise and non-repetitive paragraphs. If there is no relevant information from the context or metadata, say only 'No info'.\n",
    "Context:\n",
    "{context}\n",
    "Metadata:\n",
    "{metadata}\n",
    "Context-with-metadata:\n",
    "{context_with_metadata}\n",
    "Question: \n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the RunnableSequence\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"RAG chain ready...\")\n",
    "\n",
    "# Instantiate the custom Runnable with your retriever\n",
    "retrieve_and_format = RetrieveAndFormat(ensemble_retriever)\n",
    "\n",
    "\n",
    "# Retrieve context and metadata - sent the question to the LLM \n",
    "retrieval_output = retrieve_and_format.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "\n",
    "# Generate the answer using the complete chain. Answer is streamed.\n",
    "res = chain.stream(retrieval_output)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"----------------------\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "#Print answer to the question by LLM\n",
    "chunks = []\n",
    "print(\"Answer: \")\n",
    "for chunk in res:\n",
    "    chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "#Get the answer of the LLM and look for retrieved chunks firstly. Then, apply similarity search to understand LLM chose which chunks and docs\n",
    "res_string = ''.join(chunks)\n",
    "\n",
    "documents= retrieval_output[\"context_with_metadata\"].split(\"\\n\\n\") #These are doc chunks with metadata (aka. book paths)\n",
    "llm_output = res_string  #LLM answer\n",
    "\n",
    "\n",
    "# # Keyword Matching\n",
    "# matched_docs_keywords = keyword_matching(llm_output, documents)\n",
    "# print(\"Documents matched by keyword:\", len(matched_docs_keywords))\n",
    "\n",
    "# Similarity Assessment\n",
    "matched_docs_similarity = similarity_assessment(llm_output, documents, threshold=0.2) #Find similarities between LLM answer and retrieved text chunks\n",
    "\n",
    "\n",
    "# Initialize an empty set to track seen sources\n",
    "seen_sources = set()\n",
    "\n",
    "# Initialize a list to store unique metadata entries\n",
    "unique_metadata = []\n",
    "\n",
    "# Iterate over the retrieved documents: If multiple chunks are obtained from the same document, only that document will be printed as references\n",
    "counter=1\n",
    "for doc in matched_docs_similarity:\n",
    "    # Extract the 'source' metadata, defaulting to 'Unknown' if not present\n",
    "    source = doc.split(\"\\n\")[-1]\n",
    "    # If this source has not been encountered before, add it to the set and list\n",
    "    if source not in seen_sources:\n",
    "        seen_sources.add(source)\n",
    "        unique_metadata.append(f\"Source-{str(counter)}: {source}\")\n",
    "        counter += 1    \n",
    "\n",
    "unique_metadata = \"\\n\\n\".join(unique_metadata)\n",
    "print(f\"\\n\\nRelevant documents LLM model used:\\n\\n{unique_metadata}\") \n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"----------------------\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "#Print all relevant documents according to the question -- for testing purpose\n",
    "ref_docs =  retrieval_output['metadata']\n",
    "print(f\"All relevant documents:\\n\\n{ref_docs}\")    \n",
    "\n",
    "#Access the metadata and print that\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"----------------------\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "#Print all relevant doc chunks with metadata (aka. book paths) -- for testing purpose\n",
    "print(retrieval_output[\"context_with_metadata\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#context =  retrieval_output['context']\n",
    "#print(f\"\\nRelevant context and documents:\\n\\n{context}\")\n",
    "\n",
    "#ref_docs =  retrieval_output['metadata']\n",
    "#print(f\"\\nRelevant context and documents:\\n\\n{list(ref_docs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
